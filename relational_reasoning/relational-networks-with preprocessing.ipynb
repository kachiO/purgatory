{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relational Networks Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of Santoro, Raposo et al [A simple neural network module for relational reasoning](https://arxiv.org/pdf/1706.01427.pdf)\n",
    "\n",
    "I will start with the Visual QA dataset [CLEVR](https://cs.stanford.edu/people/jcjohns/clevr/). The dataset consists of the following: \n",
    "\n",
    "- training set: 70,000 images and 699,989 questions\n",
    "- validation set: 15,000 images and 149,991 questions\n",
    "- test set: 15,000 images and 14,988 questions\n",
    "- scene graph annotations for training and validations images including ground-truth locations, attributes, and relationships for objects\n",
    "\n",
    "The network used to perform relational reasoning consists of: \n",
    "- 4 layer convolution to process the images\n",
    "    - 24 - 3 x 3 kernels, with a stride= 2, RELU activation, and batch normalization\n",
    "- 128 unit LSTM to process questions\n",
    "- 32 unit word embedding layer following the LSTM\n",
    "- Relational network, composed of 2 multilayer perceptrons,and RELU activation: \n",
    "   - MLP #1 (g$_\\theta$): 4 dense layers with 256 units each\n",
    "   - MLP #2 (f$_\\theta$): 3 dense layers 256, 256, 29 units respectively\n",
    "- Dropout before final layer\n",
    "- Final layer linear layer that produced logits for a softmax over the answer vocabulary (softmax output). \n",
    "    - cross-entropy loss function using the Adam optimizer with a learning rate of 2.5eâˆ’4\n",
    "    - 64 mini-batches\n",
    "   \n",
    "   \n",
    "Try next:\n",
    "- 1-D conv for text processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.layers import LSTM,GRU,Conv2D,SeparableConv2D,Embedding,Dense,Input,BatchNormalization, \\\n",
    "                         Reshape,Flatten,Dropout,Lambda,RepeatVector,Concatenate,Add,TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model, Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.set_image_dim_ordering('tf')\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../../Data/CLEVR/CLEVR_v1.0/scenes/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "cwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = '/home/odenigborig/Data/CLEVR/CLEVR_v1.0/'\n",
    "images = os.path.join(data_dir,'images')\n",
    "questions = os.path.join(data_dir,'questions')\n",
    "scenes = os.path.join(data_dir,'scenes')\n",
    "\n",
    "train_images_dir = os.path.join(images,'train')\n",
    "valid_images_dir = os.path.join(images,'val')\n",
    "test_images_dir = os.path.join(images,'test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_qs = json.load(open(os.path.join(questions,'CLEVR_train_questions.json')))\n",
    "#valid_qs = json.load(open(os.path.join(questions,'CLEVR_val_questions.json')))\n",
    "#test_qs = json.load(open(os.path.join(questions,'CLEVR_test_questions.json')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_qs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_qs['questions'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_qs['questions'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index = 890\n",
    "#print(train_qs['questions'][index]['question'].encode('utf-8'))\n",
    "#print(train_qs['questions'][index]['answer'].encode('utf-8'))\n",
    "#img_name = train_qs['questions'][index]['image_filename']\n",
    "#print('image name: ' + img_name)\n",
    "\n",
    "#img = mpimg.imread(os.path.join(train_images_dir,img_name))\n",
    "#this_img= img[:,:,:3]\n",
    "#plt.imshow(this_img)\n",
    "#plt.axis('off')\n",
    "\n",
    "#plt.figure()\n",
    "#plt.imshow(img)\n",
    "#plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create lists of questions, filenames, and other pertinent details\n",
    "#train_questions = [value['question'].encode('utf-8') for counter, value in enumerate(train_qs['questions'])]\n",
    "#train_answers = [value['answer'].encode('utf-8') for counter, value in enumerate(train_qs['questions'])]\n",
    "#train_img_fnames = [value['image_filename'].encode('utf-8') for counter,value in enumerate(train_qs['questions'])]\n",
    "\n",
    "#valid_questions = [value['question'].encode('utf-8') for counter, value in enumerate(valid_qs['questions'])]\n",
    "#valid_answers = [value['answer'].encode('utf-8') for counter, value in enumerate(valid_qs['questions'])]\n",
    "#valid_img_fnames = [value['image_filename'].encode('utf-8') for counter,value in enumerate(valid_qs['questions'])]\n",
    "\n",
    "#test_questions = [value['question'].encode('utf-8') for counter, value in enumerate(test_qs['questions'])]\n",
    "#test_img_fnames = [value['image_filename'].encode('utf-8') for counter,value in enumerate(test_qs['questions'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_q_len = [len(value) for counter,value in enumerate(train_questions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max(train_q_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.unique(train_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(train_questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.unique(train_answers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN module\n",
    "- 4 layer convolution to process the images\n",
    "    - 24 - 3 x 3 kernels, with a stride= 2, RELU activation, and batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cnn module\n",
    "cnn_in = Input(shape=(128,128,3),name='img_in')\n",
    "\n",
    "conv = Conv2D(filters=24,kernel_size=(3,3),strides=2,activation='relu',name='conv_1',padding='same')(cnn_in)\n",
    "conv = BatchNormalization(axis=1,name='bn_1')(conv)\n",
    "\n",
    "for i in range(3):\n",
    "    conv = Conv2D(filters=24,kernel_size=(3,3),strides=2,activation='relu',name='conv_'+str(i+2),padding='same')(conv)\n",
    "    conv = BatchNormalization(axis=1,name='bn_'+str(i+2))(conv)\n",
    "\n",
    "cnn_module = Model(cnn_in,conv,name='cnn_module')\n",
    "cnn_module.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN module\n",
    "- 32 unit word embedding layer \n",
    "- 128 unit LSTM to process questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RNN module\n",
    "max_sequence_len = 213 #max(train_q_len)+10\n",
    "\n",
    "#rnn input shape = (batch size, time steps, dimensions)\n",
    "question_input = Input(shape=(max_sequence_len,),name='question_in')\n",
    "print(question_input._keras_shape)\n",
    "embed = Embedding(input_dim=max_sequence_len,output_dim=32,name='embedding')(question_input)\n",
    "print(embed.shape)\n",
    "lstm = LSTM(units=128,name='lstm',use_bias=False)(embed)\n",
    "print(lstm.shape)\n",
    "\n",
    "rnn_module = Model(question_input,lstm,name='rnn_module')\n",
    "rnn_module.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relational network (RN) module \n",
    "Relational network, composed of 2 multilayer perceptrons,and RELU activation:\n",
    "- MLP #1 (g$_\\theta$): 4 dense layers with 256 units each\n",
    "- MLP #2 (f$_\\theta$): 3 dense layers 256, 256, 29 units respectively\n",
    "\n",
    "The first MLP (g$_\\theta$) acts on **pairs** of objects. The output of the cnn module is the object set *O* and the object pairs are obtained from *O*. Based on the cnn module above, there are 64 object pairs (8x8). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create object-object pair and object-object-question pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sub2ind(array_shape, rows, cols):\n",
    "    return rows*array_shape[1] + cols\n",
    "\n",
    "def pair_objects(x):\n",
    "    in_shapes = K.int_shape(x)\n",
    "\n",
    "    #convert to objects matrix, D of size  m by n\n",
    "    # m objects, and each object consists of n vectors (or features) which describe properties of the object\n",
    "    num_objects = in_shapes[1]\n",
    "\n",
    "    pairs = [] \n",
    "    for i in range(num_objects):\n",
    "        for j in range(num_objects):\n",
    "            ind = sub2ind((num_objects,num_objects),i,j)\n",
    "            pairs.append(K.concatenate([x[:,i,:],x[:,j,:]]))\n",
    "\n",
    "    output = K.stack(pairs,axis=1)\n",
    "\n",
    "    return output\n",
    "\n",
    "def sum_objects(x):\n",
    "    output = K.sum(x,axis=1)\n",
    "    return output  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_shape = K.int_shape(conv)\n",
    "conv_objects = Reshape((conv_shape[1]**2,conv_shape[3]))(conv)\n",
    "object_pairs = Lambda(pair_objects,name='object_pair')(conv_objects)\n",
    "print('object-object pair shape: {}'.format(K.int_shape(object_pairs)))\n",
    "\n",
    "num_objects = (conv_shape[1]**2)\n",
    "question_embed = RepeatVector(num_objects**2,name='repeat_q_embed')(lstm)\n",
    "print('question embeddings shape: {}'.format(K.int_shape(question_embed)))\n",
    "\n",
    "object_question_pairs = Concatenate(axis=-1,name='object_question_pair')([object_pairs,question_embed])\n",
    "print('object pair-question shape: {}'.format(K.int_shape(object_question_pairs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relational network module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative approach, using TimeDistributed Layer\n",
    "\n",
    "shape_in = K.int_shape(object_question_pairs)[1:]\n",
    "\n",
    "#sequential model \n",
    "g_mlp_layers = Sequential()\n",
    "g_mlp_layers.add(Dense(units=256,activation='relu',name='g_theta_1',input_shape=(None,shape_in[1])))\n",
    "g_mlp_layers.add(Dense(units=256,activation='relu',name='g_theta_2'))\n",
    "g_mlp_layers.add(Dense(units=256,activation='relu',name='g_theta_3'))\n",
    "g_mlp_layers.add(Dense(units=256,activation='relu',name='g_theta_4'))\n",
    "\n",
    "#apply g_theta MLP to each object-question pair (i.e. row)\n",
    "g_MLP_obj_q_pairs = TimeDistributed(g_mlp_layers,name='g_theta')(object_question_pairs)\n",
    "print(g_MLP_obj_q_pairs.shape)\n",
    "\n",
    "#apply element-wise sum\n",
    "g_MLP_sum = Lambda(sum_objects,name='g_theta_sum')(g_MLP_obj_q_pairs)\n",
    "print(g_MLP_sum.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mlp 2\n",
    "f_MLP = Dense(units=256,activation='relu',name='f_theta_1')(g_MLP_sum)\n",
    "f_MLP = Dense(units=256,activation='relu',name='f_theta_2')(f_MLP)\n",
    "f_MLP = Dropout(rate=0.5,name='dropout')(f_MLP)\n",
    "rn_out = Dense(units=29,activation='softmax',name='output')(f_MLP)\n",
    "print(rn_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def g_MLP(x):\n",
    "    '''\n",
    "    x: object question relations\n",
    "    '''\n",
    "    mlp = Dense(units=256,activation='relu',name='g_theta_1')(x)\n",
    "    mlp = Dense(units=256,activation='relu',name='g_theta_2')(mlp)\n",
    "    mlp = Dense(units=256,activation='relu',name='g_theta_3')(mlp)\n",
    "    mlp_out = Dense(units=256,activation='relu',name='g_theta_4')(mlp)\n",
    "    \n",
    "    return mlp_out\n",
    "\n",
    "def g_MLP_layer(x):\n",
    "    '''\n",
    "    apply the g_theta MLP to each object-question pair separately, followed by elementwise sum\n",
    "    the parameters are shared.\n",
    "    \n",
    "    x: object question relations\n",
    "    '''\n",
    "    \n",
    "    transformed_pairs = []\n",
    "\n",
    "    for p in range(K.int_shape(x)[1]):\n",
    "        transformed_pairs.append(g_MLP(x[:,p,:]))\n",
    "\n",
    "    #element-wise sum    \n",
    "    output = Add(name='sum_g_theta')(transformed_pairs)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##apply the g_theta MLP to each object-question pair separately, this is very slow\n",
    "#g_MLP_obj_q_pairs = []\n",
    "\n",
    "#for p in range(K.int_shape(object_question_pairs)[1]):\n",
    "#    g_MLP_obj_q_pairs.append(g_MLP(object_question_pairs[:,p,:]))\n",
    "\n",
    "##element-wise sum    \n",
    "#g_MLP_sum = Add(name='sum_g_theta')(g_MLP_obj_q_pairs)\n",
    "\n",
    "#print(g_MLP_sum.shape)\n",
    "\n",
    "#g_MLP = Lambda(g_MLP_layer,name='g_MLP')(object_question_pairs)\n",
    "#print(g_MLP.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#object_question_pair_input = Input(shape=(K.int_shape(object_question_pairs)[1:]),name='object_question_in')\n",
    "#mlp 1\n",
    "#mlp_1 = Dense(units=256,activation='relu',name='g_theta_1')(object_question_pairs)\n",
    "#mlp_1 = Dense(units=256,activation='relu',name='g_theta_2')(mlp_1)\n",
    "#mlp_1 = Dense(units=256,activation='relu',name='g_theta_3')(mlp_1)\n",
    "#mlp_1 = Dense(units=256,activation='relu',name='g_theta_4')(mlp_1)\n",
    "#print(K.int_shape(mlp_1))\n",
    "\n",
    "#element-wise sum \n",
    "#mlp_1_sum = Lambda(sum_objects,name='sum_objects')(mlp_1)\n",
    "#print(K.int_shape(mlp_1_sum))\n",
    "\n",
    "#mlp 2\n",
    "#mlp_2 = Dense(units=256,activation='relu',name='f_theta_1')(mlp_1_sum)\n",
    "#mlp_2 = Dense(units=256,activation='relu',name='f_theta_2')(mlp_2)\n",
    "#mlp_2 = Dropout(rate=0.5)(mlp_2)\n",
    "#rel_out = Dense(units=29,activation='softmax',name='output')(mlp_2)\n",
    "#mlp_2 = Dense(units=29,activation='relu',name='f_theta_3')(mlp_2)\n",
    "#print(K.int_shape(mlp_2))\n",
    "\n",
    "#rel_out = Dense(units=1,activation='softmax',name='output')(mlp_2)\n",
    "#print(K.int_shape(rel_out))\n",
    "\n",
    "#rel_module = Model(object_question_pair_input,rel_out,name='rel_module')\n",
    "#rel_module.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine modules: vqa network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vqa_model  = Model([cnn_in,question_input],rn_out,name='vqa_model')\n",
    "vqa_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess text into sequences\n",
    "- Tokenize: convert text into integers\n",
    "- Pad sequences to the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tokenizer = Tokenizer(num_words=1000)\n",
    "#tokenizer.fit_on_texts(train_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(len(tokenizer.word_counts))\n",
    "#word_counts = tokenizer.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_questions_sequences = tokenizer.texts_to_sequences(train_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#index = 1100\n",
    "#print(train_questions[index])\n",
    "#print(train_questions_sequences[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pad sequences with zeros at the end\n",
    "#train_sequences = pad_sequences(train_questions_sequences,maxlen=max_sequence_len,padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#index = np.random.randint(len(train_questions))\n",
    "#print('Question:')\n",
    "#print(train_questions[index])\n",
    "#print('')\n",
    "#print('Sequence:')\n",
    "#print(train_questions_sequences[index])\n",
    "#print('')\n",
    "#print('Sequence-Padded:')\n",
    "#print(train_sequences[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def process_text(text,num_words=1000,maxlen=None,return_tokenizer=False):\n",
    "#    tokenizer = Tokenizer(num_words=1000)\n",
    "#    tokenizer.fit_on_texts(text)\n",
    "    \n",
    "#    sequences = tokenizer.texts_to_sequences(text)\n",
    "    \n",
    "#    sequences = pad_sequences(sequences,maxlen=maxlen,padding='post',truncating='post')\n",
    "\n",
    "#    if return_tokenizer:    \n",
    "#        return sequences, tokenizer\n",
    "#    else:\n",
    "#        return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train_answers_num = process_text(train_answers)\n",
    "\n",
    "#valid_sequences = process_text(valid_questions,maxlen=max_sequence_len)\n",
    "#valid_answers_num = process_text(valid_answers)\n",
    "\n",
    "#test_sequences = process_text(test_questions,maxlen=max_sequence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print('Train answers: {}'.format(np.unique(train_answers_num)))\n",
    "#print('')\n",
    "#print('Valid: {}'.format(np.unique(valid_sequences)))\n",
    "#print('')\n",
    "#print('Valid qs:{}'.format(np.unique(valid_answers_num)))\n",
    "#print('')\n",
    "\n",
    "#print('Test: {}'.format(np.unique(test_sequences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save sequences \n",
    "#clear memory\n",
    "del train_qs,valid_qs,test_qs,train_questions,valid_questions,test_questions\n",
    "\n",
    "#load_train = np.load('training.npz')\n",
    "#load_valid = np.load('valid.npz')\n",
    "#load_test = np.load('testing.npz')\n",
    "\n",
    "#train_answers_num = to_categorical(train_answers_num,num_classes=29)\n",
    "#valid_answers_num = to_categorical(valid_answers_num,num_classes=29)\n",
    "\n",
    "#np.savez_compressed('train_data',question_sequences=train_sequences,\n",
    "#                    answers=train_answers_num,image_file=train_img_fnames)\n",
    "#np.savez_compressed('valid_data',question_sequences=valid_sequences,\n",
    "#                    answers=valid_answers_num,image_file=valid_img_fnames)\n",
    "#np.savez_compressed('test_data',question_sequences=test_sequences,\n",
    "#                    image_file=test_img_fnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process images\n",
    "Here, what I'll do is generate a list of images and save that. this might take a long time and potentially run out of memory. Let's see `\\_('_')_/`\n",
    "\n",
    "Yeah, that's not going to work. Instead create a data generator for the images. \n",
    "\n",
    "- preprocessing:\n",
    "    - downsample to 128 x 128\n",
    "    - remove 4th channel (alpha channel) i.e. convert to RGB\n",
    "    - rescale (divide by 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (128,128) \n",
    "img_file = train_qs['questions'][index]['image_filename'].encode('utf-8')\n",
    "this_img = Image.open(os.path.join(train_images_dir,img_file))\n",
    "this_img = np.asarray(this_img.convert('RGB').resize(img_size,Image.ANTIALIAS))/255. #convert to RGB, resize, and numpy array\n",
    "plt.imshow(this_img)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_img(path,new_size=(128,128)):\n",
    "    '''\n",
    "    Load image, convert to RGB, & resize\n",
    "    '''\n",
    "    img = Image.open(path)\n",
    "    img = img.convert('RGB').resize(new_size,Image.ANTIALIAS)\n",
    "    \n",
    "    return img\n",
    "\n",
    "def pad_img(img_in,new_size=(136,136)):\n",
    "    '''\n",
    "    Apply zero padding to PIL image object\n",
    "\n",
    "    img_in: PIL image object\n",
    "    new_size: (width,height) to be consistent with PIL Image objects\n",
    "    '''\n",
    "    \n",
    "    w,h = img_in.size[0],img_in.size[1] #returns (width,height) tuple\n",
    "    \n",
    "    h_pad = np.abs(h - new_size[0]) // 2\n",
    "    w_pad = np.abs(w - new_size[1]) // 2\n",
    "    \n",
    "    new_img = Image.new('RGB',new_size)\n",
    "    \n",
    "    #4 element tuple defining the left, upper, right, and lower pixel coordinate\n",
    "    coordinates = (w_pad, h_pad, w_pad+w, h_pad+h)\n",
    "    new_img.paste(img_in,coordinates)\n",
    "    \n",
    "    return new_img\n",
    "    \n",
    "def random_crop(img_in,crop_size=(128,128),seed=None):\n",
    "    '''\n",
    "    Randomly crop PIL image object\n",
    "    Assumes 'channels last' data format. \n",
    "    \n",
    "    img_in: PIL image object\n",
    "    crop_size: (width,height) to be consistent with PIL Image objects\n",
    "    seed  : random seed\n",
    "    '''\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "\n",
    "    w,h = img_in.size[0],img_in.size[1]\n",
    "    \n",
    "    w_range = np.abs(w - crop_size[0]) // 2\n",
    "    h_range = np.abs(h - crop_size[1]) // 2\n",
    "    \n",
    "    w_offset = 0 if w_range == 0 else np.random.randint(w_range)\n",
    "    h_offset = 0 if h_range == 0 else np.random.randint(h_range)\n",
    "    \n",
    "    w_start,w_end = w_offset, w_offset + crop_size[0]\n",
    "    h_start,h_end = h_offset, h_offset + crop_size[1]\n",
    "\n",
    "    #4 element tuple defining the left, upper, right, and lower pixel coordinate\n",
    "    coordinates = (w_start,h_start,w_end,h_end)\n",
    "    \n",
    "    img_out = img_in.crop(coordinates)\n",
    "    \n",
    "    return img_out\n",
    "    \n",
    "def random_rotate_img(img_in,rotation_range=(-2.86,2.86)):\n",
    "    '''\n",
    "    Apply random rotation from -0.05 to 0.05 radians (-2.86 to 2.86 degrees)\n",
    "    \n",
    "    img_in: PIL image object\n",
    "    rotation_range: start and end angle (in degrees) for rotation\n",
    "    '''\n",
    "    \n",
    "    angle = (rotation_range[1] - rotation_range[0])*np.random.rand() - rotation_range[0]\n",
    "    img_out = img_in.rotate(angle,Image.BILINEAR)\n",
    "    \n",
    "    return img_out\n",
    "\n",
    "\n",
    "def pil_img_to_array(img_in,rescale=True):\n",
    "    '''\n",
    "    Conver PIL image to numpy array\n",
    "    '''\n",
    "    img_out = np.asarray(img_in)\n",
    "    \n",
    "    if rescale:\n",
    "        img_out = img_out / 255.\n",
    "\n",
    "    return img_out\n",
    "\n",
    "def process_images(img, augment=True):\n",
    "    if augment:\n",
    "        img = pad_img(img)\n",
    "        img = random_crop(img)\n",
    "        img = random_rotate_img(img)\n",
    "    \n",
    "    img_out = pil_img_to_array(img)\n",
    "    \n",
    "    return img_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check image transformation functions\n",
    "index = np.random.randint(len(train_img_fnames))\n",
    "img = load_img(os.path.join(train_images_dir,train_img_fnames[index]))\n",
    "print(img.size)\n",
    "plt.figure()\n",
    "plt.imshow(pil_img_to_array(img))\n",
    "plt.title('image: '+ train_img_fnames[index])\n",
    "plt.axis('off')\n",
    "\n",
    "img = pad_img(img)\n",
    "print(img.size)\n",
    "plt.figure()\n",
    "plt.imshow(pil_img_to_array(img))\n",
    "plt.title('padding: '+ train_img_fnames[index])\n",
    "plt.axis('off')\n",
    "\n",
    "img = random_crop(img)\n",
    "print(img.size)\n",
    "plt.figure()\n",
    "plt.imshow(pil_img_to_array(img))\n",
    "plt.title('random_crop: '+ train_img_fnames[index])\n",
    "plt.axis('off')\n",
    "\n",
    "img = random_rotate_img(img)\n",
    "print(img.size)\n",
    "plt.figure()\n",
    "plt.imshow(pil_img_to_array(img))\n",
    "plt.title('random_rotate: '+ train_img_fnames[index])\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = load_img(os.path.join(train_images_dir,train_img_fnames[index]))\n",
    "img = process_images(img)\n",
    "print(img.shape)\n",
    "plt.imshow(img)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Data Generator Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generator class\n",
    "class VQDataGenerator(object):\n",
    "    '''\n",
    "    Generate data for model i.e. batch of questions and images\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,n=100,dim_x_img=128,dim_y_img=128,img_dir=None,sequence_len=213,num_classes=29,batch_size=32,shuffle=True,augment=True):\n",
    "        '''\n",
    "        Initialize class\n",
    "        '''\n",
    "        self.n = n\n",
    "        self.dim_x_img = dim_x_img\n",
    "        self.dim_y_img = dim_y_img\n",
    "        self.img_dir = img_dir\n",
    "        self.sequence_len = sequence_len\n",
    "        self.num_classes = num_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.augment = augment\n",
    "        \n",
    "    def __get_data_order(self):\n",
    "        '''\n",
    "        get indices i.e. order of samples\n",
    "        '''\n",
    "        indexes = np.arange(self.n)\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(indexes)\n",
    "        return indexes\n",
    "            \n",
    "    def __data_generation(self,batch_inds,data_dict):\n",
    "        '''\n",
    "        Generate batches of data of batch_size:   \n",
    "        '''       \n",
    "        \n",
    "        batch_imgs = np.empty((self.batch_size,self.dim_y_img,self.dim_x_img,3))\n",
    "        batch_qtns = np.empty((self.batch_size,self.sequence_len),dtype=int)\n",
    "        batch_anrs = np.empty((self.batch_size,self.num_classes),dtype=int)\n",
    "        \n",
    "        for i,val in enumerate(batch_inds):\n",
    "            this_img_file = os.path.join(self.img_dir,data_dict['image_file'][val])\n",
    "            batch_imgs[i,:,:,:] = process_images(load_img(this_img_file),self.augment)\n",
    "            batch_qtns[i,:] = data_dict['question_sequences'][val,:]\n",
    "            batch_anrs[i,:] = data_dict['answers'][val]\n",
    "            \n",
    "        return batch_imgs,batch_qtns,batch_anrs\n",
    "        \n",
    "    def generate(self,data_dict):\n",
    "        '''\n",
    "        Generate batches of samples\n",
    "        '''        \n",
    "        while True:\n",
    "            #Generate order of dataset\n",
    "            indexes = self.__get_data_order()\n",
    "            \n",
    "            #Generate batches\n",
    "            imax = len(indexes) // self.batch_size\n",
    "            for i in range(imax):\n",
    "                temp_ids = [k for k in indexes[i*self.batch_size:(i+1)*self.batch_size]]\n",
    "                #generate data\n",
    "                batch_images,batch_questions,batch_answers = self.__data_generation(temp_ids,data_dict)\n",
    "                \n",
    "                X = [batch_images,batch_questions]\n",
    "                y = batch_answers\n",
    "                \n",
    "                yield (X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data files\n",
    "train_data = np.load('train_data.npz')\n",
    "valid_data = np.load('valid_data.npz')\n",
    "test_data = np.load('test_data.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train_samples = len(train_data['image_file'])\n",
    "n_valid_samples = len(valid_data['image_file'])\n",
    "n_test_samples = len(test_data['image_file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_train = {'n':n_train_samples,\n",
    "                'dim_x_img': 128,\n",
    "                'dim_y_img': 128,\n",
    "                'img_dir': train_images_dir,\n",
    "                'sequence_len': 213,\n",
    "                'num_classes': 29,\n",
    "                'batch_size': 32,\n",
    "                'shuffle': True,\n",
    "                'augment': True}\n",
    "\n",
    "params_valid = {'n': n_valid_samples,\n",
    "                'dim_x_img': 128,\n",
    "                'dim_y_img': 128,\n",
    "                'img_dir': valid_images_dir,\n",
    "                'sequence_len': 213,\n",
    "                'num_classes': 29,\n",
    "                'batch_size': 32,\n",
    "                'shuffle': True,\n",
    "                'augment': False}\n",
    "\n",
    "train_generator = VQDataGenerator(**params_train).generate(train_data)\n",
    "valid_generator = VQDataGenerator(**params_valid).generate(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call next iteration i.e. batch\n",
    "gen_flow = valid_generator.next()\n",
    "batch_imgs,batch_questions = gen_flow[0][0], gen_flow[0][1]\n",
    "batch_answers = gen_flow[1]\n",
    "\n",
    "f,axarrs = plt.subplots(2,2,figsize=(20,8))\n",
    "axs = axarrs.ravel()\n",
    "\n",
    "for a in range(len(axs)):\n",
    "    axs[a].imshow(batch_imgs[a])\n",
    "    axs[a].set_title(str(a+1))\n",
    "    axs[a].axis('off')\n",
    "    print(str(a+1))\n",
    "    print(batch_questions[a,:])\n",
    "    print(batch_answers[a,:])\n",
    "    print('')\n",
    "\n",
    "print(batch_imgs.shape)\n",
    "print(batch_answers.shape)\n",
    "print(batch_questions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_flow =train_generator.next()\n",
    "batch_imgs,batch_questions = gen_flow[0][0], gen_flow[0][1]\n",
    "batch_answers = gen_flow[1]\n",
    "\n",
    "f,axarrs = plt.subplots(2,2,figsize=(20,8))\n",
    "axs = axarrs.ravel()\n",
    "\n",
    "for a in range(len(axs)):\n",
    "    axs[a].imshow(batch_imgs[a])\n",
    "    axs[a].set_title(str(a+1))\n",
    "    axs[a].axis('off')\n",
    "    print(str(a+1))\n",
    "    print(batch_questions[a,:])\n",
    "    print(batch_answers[a,:])\n",
    "    print('')\n",
    "\n",
    "print(batch_imgs.shape)\n",
    "print(batch_answers.shape)\n",
    "print(batch_questions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del batch_imgs, batch_questions, batch_answers, gen_flow, train_generator, valid_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpt_dir = '/home/odenigborig/Github/relational_reasoning/models/'\n",
    "checkpt_file = checkpt_dir+'vqa_model.h5'\n",
    "log_dir = '/home/odenigborig/Github/relational_reasoning/log_dir/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32 #64 batches used in the paper\n",
    "epochs = 1e5 #1.4e6 #in the paper they went to a 1.4 million!!\n",
    "patience = epochs\n",
    "lr = 2.5e-4/float(64/float(batch_size)) #divide by ratio of batch size. paper used batch size of 64, hence divide by 4\n",
    "\n",
    "\n",
    "callbacks_list = [ModelCheckpoint(filepath=checkpt_file,monitor='val_loss',save_best_only=True),\n",
    "                  ReduceLROnPlateau(monitor='val_loss',factor=0.1,patience=patience)]\n",
    "#                 EarlyStopping(monitor='acc',patience=patience),\n",
    "#                 TensorBoard(log_dir=log_dir,batch_size=batch_size,histogram_freq=20,embeddings_freq=20)]\n",
    "\n",
    "#authors used cross entropy loss function \n",
    "vqa_model  = Model([cnn_in,question_input],rn_out,name='vqa_model')\n",
    "vqa_model.compile(Adam(lr=lr),loss='categorical_crossentropy',metrics=['acc'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params_train = {'n':n_train_samples,\n",
    "                'dim_x_img': 128,\n",
    "                'dim_y_img': 128,\n",
    "                'img_dir': train_images_dir,\n",
    "                'sequence_len': max_sequence_len,\n",
    "                'num_classes': 29,\n",
    "                'batch_size': batch_size,\n",
    "                'shuffle': True,\n",
    "                'augment': True}\n",
    "\n",
    "params_valid = {'n': n_valid_samples,\n",
    "                'dim_x_img': 128,\n",
    "                'dim_y_img': 128,\n",
    "                'img_dir': valid_images_dir,\n",
    "                'sequence_len': max_sequence_len,\n",
    "                'num_classes': 29,\n",
    "                'batch_size': batch_size,\n",
    "                'shuffle': True,\n",
    "                'augment': False}\n",
    "\n",
    "train_generator = VQDataGenerator(**params_train).generate(train_data)\n",
    "valid_generator = VQDataGenerator(**params_valid).generate(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = vqa_model.fit_generator(train_generator,epochs=epochs,verbose=1,\n",
    "                                  callbacks=callbacks_list,\n",
    "                                  steps_per_epoch=n_train_samples//batch_size,\n",
    "                                  validation_data=valid_generator,\n",
    "                                  validation_steps=n_valid_samples//batch_size)\n",
    "                                  #use_multiprocessing=True,workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_loss(history_model):\n",
    "    f,(ax1,ax2)=plt.subplots(2,1,sharex=True)\n",
    "    ax1.plot(history_model.history['val_loss'],label='valid')\n",
    "    ax1.plot(history_model.history['loss'],label='train')\n",
    "    ax2.plot(history_model.history['val_acc'],label='valid')\n",
    "    ax2.plot(history_model.history['acc'],label='train')\n",
    "\n",
    "    ax1.legend()\n",
    "    ax2.legend()\n",
    "    plt.xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "\n",
    "def print_score(model,X,angle,y):\n",
    "    score = model.evaluate([X, angle], y, verbose=1)\n",
    "    print('')\n",
    "    print('Hold out score:', score[0])\n",
    "    print('Hold out accuracy(%):', 100*score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do:\n",
    "\n",
    "<s>1. image preprocessing/data augmentation:\n",
    "    - preprocessing:\n",
    "        - downsample to 128 x 128\n",
    "        - remove 4th channel (might be blank?)\n",
    "    - augmentations:\n",
    "        - pad to 136 x 136\n",
    "        - random cropping back to to 128 x 128\n",
    "        - random rotation from -0.05 to 0.05 radians (-2.86 to 2.86 degrees)\n",
    "2. text preprocessing:\n",
    "    - tokenize (convert to integers)\n",
    "    - make the same length\n",
    "3. write data generator to apply image augmentations and pair questions with images\n",
    "    - makes more sense to start with questions, and find corresponding image. </s>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
