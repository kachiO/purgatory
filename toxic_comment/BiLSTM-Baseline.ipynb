{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bidirectional LSTM as implemented by Jeremy Howard's [kernel](https://www.kaggle.com/jhoward/improved-lstm-baseline-glove-dropout/notebook)\n",
    " -  To do: try out concat pooling method\n",
    " - implement in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-21T13:07:10.860496Z",
     "start_time": "2018-05-21T13:07:10.854050Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM,Embedding,Dropout,Activation, Bidirectional,GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-21T13:07:23.812079Z",
     "start_time": "2018-05-21T13:07:23.807689Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow\n"
     ]
    }
   ],
   "source": [
    "print(K.backend())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download GloVe pretrained embedding from https://nlp.stanford.edu/projects/glove/, specifically the `glove.6B.zip` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-21T13:14:39.768450Z",
     "start_time": "2018-05-21T13:14:39.763908Z"
    }
   },
   "outputs": [],
   "source": [
    "PATH = '/home/odenigborig/Data/kaggle/toxic_comment'\n",
    "train_file = os.path.join(PATH,'train.csv')\n",
    "test_file = os.path.join(PATH,'test.csv')\n",
    "embedding_file = os.path.join(PATH,'glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-21T13:17:28.684631Z",
     "start_time": "2018-05-21T13:17:28.681055Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_size = 50   #size of word vector embedding\n",
    "max_words = 20000 #number of unique words\n",
    "max_len = 100     #sequence length, number of words to use in a comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load text data and replace missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-21T13:46:07.790442Z",
     "start_time": "2018-05-21T13:46:05.806647Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(train_file)\n",
    "test = pd.read_csv(test_file)\n",
    "\n",
    "list_sentences_train = train['comment_text'].fillna('_na_').values\n",
    "list_sentences_test = test['comment_text'].fillna('_na_').values\n",
    "list_classes = list(train.columns[2:])\n",
    "y = train[list_classes].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-21T13:46:17.961762Z",
     "start_time": "2018-05-21T13:46:17.957124Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 6)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the texts by turning into tokens then list of word indices of equal length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-21T13:50:08.115878Z",
     "start_time": "2018-05-21T13:49:24.621382Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(list_sentences_train)\n",
    "list_tokenizer_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenizer_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_train = pad_sequences(list_tokenizer_train,maxlen=max_len)\n",
    "X_test = pad_sequences(list_tokenizer_test,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-21T13:51:06.302632Z",
     "start_time": "2018-05-21T13:51:06.297427Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((159571, 100), (153164, 100))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read glove word vectors into dictionary, mapping words to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-21T13:54:28.201006Z",
     "start_time": "2018-05-21T13:54:20.697313Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_coefs(word,*arr): return word,np.asarray(arr,dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(embedding_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the above vectors to create embedding matrix with random initialization for words not in GloVe. Use the same mean and st. dev of GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-21T14:26:37.138048Z",
     "start_time": "2018-05-21T14:26:35.948941Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.020940464, 0.64410418)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_embeddings = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_stdev = all_embeddings.mean(),all_embeddings.std()\n",
    "emb_mean,emb_stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-21T22:53:07.365769Z",
     "start_time": "2018-05-21T22:53:06.710539Z"
    }
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_words,len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean,emb_stdev,(nb_words,embed_size)) #randomly initialize embedding matrix\n",
    "\n",
    "#assign GloVe vectors to known words (i.e. words in Glove), and random vectors to unknown words\n",
    "for word,i in word_index.items():\n",
    "    if i >= max_words: continue #exit for loop\n",
    "    embedding_vector = embeddings_index.get(word) #retrieve GloVe vector\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector  #assign GloVe vector to embedding matrix if word is present\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model: Bidirectional LSTM with two fully connected layers. Add dropout because of overfitting after 2 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-21T23:24:18.255846Z",
     "start_time": "2018-05-21T23:24:17.048215Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 100, 50)           1000000   \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 100, 100)          40400     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 1,045,756\n",
      "Trainable params: 1,045,756\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inp = Input(shape=(max_len,))\n",
    "x = Embedding(max_words,embed_size,weights=[embedding_matrix])(inp)\n",
    "x = Bidirectional(LSTM(50,return_sequences=True,dropout=0.1,recurrent_dropout=0.1))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(50,activation='relu')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(6,activation='sigmoid')(x)\n",
    "model = Model(inp,x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-21T23:57:21.194197Z",
     "start_time": "2018-05-21T23:34:54.122427Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143613/143613 [==============================] - 664s 5ms/step - loss: 0.0584 - acc: 0.9799 - val_loss: 0.0479 - val_acc: 0.9821\n",
      "Epoch 2/2\n",
      "143613/143613 [==============================] - 680s 5ms/step - loss: 0.0442 - acc: 0.9832 - val_loss: 0.0465 - val_acc: 0.9832\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f130903ffd0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.fit(X_train,y,batch_size=32,epochs=2,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-22T00:35:00.442419Z",
     "start_time": "2018-05-22T00:33:42.961878Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "\n",
    "df_submission = pd.read_csv(os.path.join(PATH,'sample_submission.csv'))\n",
    "kag_preds = model.predict(X_test,batch_size=1024,verbose=1)\n",
    "df_submission[df_submission.columns[1:]] = kag_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-22T00:35:51.460686Z",
     "start_time": "2018-05-22T00:35:51.437519Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>0.995608</td>\n",
       "      <td>3.975916e-01</td>\n",
       "      <td>0.969701</td>\n",
       "      <td>1.437341e-01</td>\n",
       "      <td>0.903331</td>\n",
       "      <td>0.246510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>9.189914e-07</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>1.845149e-07</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>0.001102</td>\n",
       "      <td>3.386773e-06</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>2.323211e-06</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>0.001666</td>\n",
       "      <td>2.777298e-06</td>\n",
       "      <td>0.000285</td>\n",
       "      <td>1.380096e-06</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>0.005767</td>\n",
       "      <td>1.857494e-05</td>\n",
       "      <td>0.000905</td>\n",
       "      <td>1.936324e-05</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.000051</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id     toxic  severe_toxic   obscene        threat    insult  \\\n",
       "0  00001cee341fdb12  0.995608  3.975916e-01  0.969701  1.437341e-01  0.903331   \n",
       "1  0000247867823ef7  0.000266  9.189914e-07  0.000077  1.845149e-07  0.000008   \n",
       "2  00013b17ad220c46  0.001102  3.386773e-06  0.000198  2.323211e-06  0.000031   \n",
       "3  00017563c3f7919a  0.001666  2.777298e-06  0.000285  1.380096e-06  0.000086   \n",
       "4  00017695ad8997eb  0.005767  1.857494e-05  0.000905  1.936324e-05  0.000220   \n",
       "\n",
       "   identity_hate  \n",
       "0       0.246510  \n",
       "1       0.000003  \n",
       "2       0.000011  \n",
       "3       0.000007  \n",
       "4       0.000051  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-22T00:46:37.823987Z",
     "start_time": "2018-05-22T00:46:36.975234Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='submission_lstm_baseline.csv' target='_blank'>submission_lstm_baseline.csv</a><br>"
      ],
      "text/plain": [
       "/home/odenigborig/Github/Kaggle/toxic_comment/submission_lstm_baseline.csv"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname = 'submission_lstm_baseline.csv'\n",
    "df_submission.to_csv(fname, index=False)\n",
    "FileLink(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "above model scored 0.9759 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:anaconda2]",
   "language": "python",
   "name": "conda-env-anaconda2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
